# üöÄ Minimal-Crawler üöÄ

[Go Producer] --> [Kafka Broker(s)] --> [Kafka Connect] --> [HDFS]

Este proyecto es un minimal crawler modular dise√±ado para capturar, procesar y almacenar datos de p√°ginas web. El sistema est√° dividido en tres componentes principales: Fetcher, Parser y Storage, cada uno con responsabilidades claramente definidas. A continuaci√≥n, encontrar√°s una explicaci√≥n detallada de c√≥mo funciona cada parte y c√≥mo se integran entre s√≠.

## üõ†Ô∏è Arquitectura del Crawler

### Fetcher üåê
El Fetcher es el componente encargado de hacer solicitudes HTTP para obtener el contenido de una p√°gina web. Su funci√≥n principal es interactuar con servidores web, manejar respuestas y gestionar errores o redirecciones.

Funciones principales:

Recibe una URL como entrada.
Realiza la solicitud HTTP para obtener el contenido de la p√°gina.
Gestiona tiempos de espera, redirecciones, errores de servidor (404, 500) y restricciones como el archivo robots.txt.
Devuelve el contenido HTML bruto para ser procesado.

### Parser üß©
El Parser se encarga de analizar el contenido obtenido por el Fetcher y extraer la informaci√≥n relevante. Se basa en herramientas con las que se puede navegar por el √°rbol DOM del HTML.

Funciones principales:

Recibe el contenido HTML como entrada.
Analiza y extrae los datos espec√≠ficos que se necesitan (t√≠tulos, enlaces, im√°genes, etc.).
Transforma los datos si es necesario (por ejemplo, limpieza de texto o normalizaci√≥n de URLs).
Devuelve la informaci√≥n en un formato estructurado, como un diccionario o JSON.

### Storage üíæ
El Storage se encarga de almacenar los datos extra√≠dos en un sistema persistente de indexers distibuido.

Funciones principales:

Recibe los datos estructurados del Parser.
Decide c√≥mo almacenar los datos.
Guarda la informaci√≥n junto con metadatos (URL, fecha de captura, etc.).
Gestiona errores en el almacenamiento (por ejemplo, duplicados o inserciones fallidas, o problemas con los indexers).