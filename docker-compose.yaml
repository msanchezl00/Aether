networks:
  aether-network:
    driver: bridge
volumes:
  kafka-logs:
  kafka-data:
  hdfs-namenode:
  hdfs-datanode-1:
  hdfs-datanode-2:

services:

  crawler:
    build:
      context: ./minimal-crawler
      dockerfile: Dockerfile
    environment:
      - CONF_SEEDS=[{"https://www.usal.es":0},{"https://www.unileon.es":0},{"https://www.uva.es":0},{"https://www.ubu.es":0}]
      - CONF_BROKERS=["kafka-broker:9092"]
      - CONF_PRODUCER_TOPIC=crawled_data
    depends_on:
      - create-topics
      - create-hdfs-connector 
    networks:
      - aether-network 

  indexer:
    build:
      context: ./minimal-indexer
      dockerfile: Dockerfile
    environment:
      - CONF_BROKERS=["kafka-broker:9092"]
      - CONF_PRODUCER_TOPIC=parquet_data
      - CONF_CONSUMER_TOPIC=crawled_data
    depends_on:
      - create-topics
      - create-hdfs-connector
    networks:
      - aether-network    

  ranking:
    build:
      context: ./minimal-ranking
      dockerfile: Dockerfile
    depends_on:
      - spark-master
    networks:
      - aether-network   
    ports:
      - "8000:8000" 

  search:
     build:
       context: ./minimal-search
       dockerfile: Dockerfile
     depends_on:
       - ranking
     networks:
       - aether-network   
     ports:
       - "5050:5050"       

  # Zookeeper es el que gestiona el flujo y comportamiento entre los brokers de kafka
  zookeeper:
    networks:
      - aether-network
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"  

  # Broker de Kafka el cual gestionara topics y particiones, gestion de publicacion y consumo de mensajes
  kafka-broker:
    networks:
      - aether-network
    image: confluentinc/cp-kafka:7.4.4
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://:9092,PLAINTEXT_HOST://:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker:9092,PLAINTEXT_HOST://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_MESSAGE_MAX_BYTES: 52428800       # 50 MB
      KAFKA_REPLICA_FETCH_MAX_BYTES: 52428800 # 50 MB
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_LOG_SEGMENT_BYTES: 134217728      # 128MB
      KAFKA_LOG_RETENTION_BYTES: 1073741824   # 1GB total antes de borrar
    ports:
      - "9092:9092"
      - "9093:9093"
    volumes:
      - kafka-logs:/var/log/kafka
      - kafka-data:/var/lib/kafka/data
  # Servicio para crear topics necesarios en Kafka
  create-topics:
    networks:
      - aether-network
    image: confluentinc/cp-kafka:latest
    depends_on:
      - kafka-broker
    entrypoint: >
      bash -c "
      echo 'Waiting for Kafka to be ready...';
      cub kafka-ready -b kafka:9092 1 20;
      echo 'Creating topics...';
      kafka-topics --bootstrap-server kafka-broker:9092 --create --topic crawled_data;
      kafka-topics --bootstrap-server kafka-broker:9092 --create --topic parquet_data;
      echo 'Topics created.'"

  # Servicio de Kafka Connect para conectar Kafka con otros sistemas como HDFS en este caso
  kafka-connect:
    networks:
      - aether-network
    image: confluentinc/cp-kafka-connect:latest
    depends_on:
      - kafka-broker
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka-broker:9092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "quickstart"
      CONNECT_CONFIG_STORAGE_TOPIC: "quickstart-config"
      CONNECT_OFFSET_STORAGE_TOPIC: "quickstart-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "quickstart-status"
      CONNECT_KEY_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "true"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "true"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components/"
    ports:
      - "8083:8083"
    command: 
      - bash
      - -c
      - |
        confluent-hub install --no-prompt confluentinc/kafka-connect-hdfs:latest && \
        /etc/confluent/docker/run
  # Servicio para crear el conector HDFS en Kafka Connect y conectarlo con nuestro HDFS
  create-hdfs-connector:
    networks:
      - aether-network
    image: curlimages/curl:latest
    depends_on:
      - kafka-connect
      - hdfs-namenode
    entrypoint: >
      sh -c "
      echo 'Esperando a Kafka Connect...';
      sleep 120;
      echo 'Creando conector HDFS...';
      curl -X POST -H 'Content-Type: application/json' --data '{
        \"name\": \"hdfs-sink\",
        \"config\": {
          \"connector.class\": \"io.confluent.connect.hdfs.HdfsSinkConnector\",
          \"tasks.max\": \"1\",
          \"topics\": \"parquet_data\",
          \"hdfs.url\": \"hdfs://hdfs-namenode:8020\",
          \"flush.size\": \"1000\",
          \"format.class\": \"io.confluent.connect.hdfs.avro.AvroFormat\",
          \"schema.registry.url\": \"http://schema-registry:8081\",
          \"storage.class\": \"io.confluent.connect.hdfs.storage.HdfsStorage\",
          \"rotate.interval.ms\": \"60000\",
          \"partitioner.class\": \"io.confluent.connect.storage.partitioner.FieldPartitioner\",
          \"partition.field.name\": \"domain,date\",
          \"path.format\": \"${domain}/${date}\",
          \"locale\": \"en\",
          \"timezone\": \"UTC\"
        }
      }' http://kafka-connect:8083/connectors;
      echo 'Conector HDFS creado.'
      "

  schema-registry:
    image: confluentinc/cp-schema-registry:latest
    depends_on:
      - kafka-broker
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka-broker:9092
    ports:
      - "8081:8081"
    networks:
      - aether-network

  # Servicio de HDFS con un namenode el cual es como el cerebro de los datanodes que almacenan los datos
  hdfs-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    networks:
      - aether-network
    environment:
      - CLUSTER_NAME=hdfs-cluster
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:8020
      - HDFS_CONF_dfs_permissions_enabled=false
    ports:
      - "9870:9870"   # Web UI
      - "8020:8020"   # RPC
    volumes:
      - hdfs-namenode:/hadoop/dfs/name
    hostname: hdfs-namenode

  # Servicio de HDFS con un datanode el cual almacena los datos realmente en bloques y responde a las solicitudes de lectura y escritura por parte del namenode
  hdfs-datanode-1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    networks:
      - aether-network
    depends_on:
      - hdfs-namenode
    environment:
      - CLUSTER_NAME=hdfs-cluster
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:8020
      - SERVICE_PRECONDITION=hdfs-namenode:8020
      - HDFS_CONF_dfs_datanode_data_dir=file:///hadoop/dfs/data
      - HDFS_CONF_dfs_permissions_enabled=false
    ports:
      - "9864:9864"
    volumes:
      - hdfs-datanode-1:/hadoop/dfs/data
    hostname: hdfs-datanode-1

  spark-master:
    image: kabileshe/bitnamispark2:3.5.1
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_SUBMIT_OPTIONS=--packages org.apache.spark:spark-avro_2.12:3.5.1
      - SPARK_CONF_DIR=/opt/spark/conf
    depends_on:
      - hdfs-namenode
    networks:
      - aether-network
    ports:
      - "8080:8080"   # UI web
      - "7077:7077"   # puerto master para workers

  spark-worker-1:
    image: kabileshe/bitnamispark2:3.5.1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_SUBMIT_OPTIONS=--packages org.apache.spark:spark-avro_2.12:3.5.1
      - SPARK_CONF_DIR=/opt/spark/conf
    networks:
      - aether-network