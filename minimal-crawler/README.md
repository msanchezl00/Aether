# ğŸš€ Minimal-Crawler ğŸš€

Este proyecto es un minimal crawler modular diseÃ±ado para capturar, procesar y almacenar datos de pÃ¡ginas web. El sistema estÃ¡ dividido en tres componentes principales: Fetcher, Parser y Storage, cada uno con responsabilidades claramente definidas. A continuaciÃ³n, encontrarÃ¡s una explicaciÃ³n detallada de cÃ³mo funciona cada parte y cÃ³mo se integran entre sÃ­.

## ğŸ› ï¸ Arquitectura del Crawler

### Fetcher ğŸŒ
El Fetcher es el componente encargado de hacer solicitudes HTTP para obtener el contenido de una pÃ¡gina web. Su funciÃ³n principal es interactuar con servidores web, manejar respuestas y gestionar errores o redirecciones.

Funciones principales:

Recibe una URL como entrada.
Realiza la solicitud HTTP para obtener el contenido de la pÃ¡gina.
Gestiona tiempos de espera, redirecciones, errores de servidor (404, 500) y restricciones como el archivo robots.txt.
Devuelve el contenido HTML bruto para ser procesado.

### Parser ğŸ§©
El Parser se encarga de analizar el contenido obtenido por el Fetcher y extraer la informaciÃ³n relevante. Se basa en herramientas con las que se puede navegar por el Ã¡rbol DOM del HTML.

Funciones principales:

Recibe el contenido HTML como entrada.
Analiza y extrae los datos especÃ­ficos que se necesitan (tÃ­tulos, enlaces, imÃ¡genes, etc.).
Transforma los datos si es necesario (por ejemplo, limpieza de texto o normalizaciÃ³n de URLs).
Devuelve la informaciÃ³n en un formato estructurado, como un diccionario o JSON.

### Storage ğŸ’¾
El Storage se encarga de almacenar los datos extraÃ­dos en un sistema persistente de indexers distibuido.

Funciones principales:

Recibe los datos estructurados del Parser.
Decide cÃ³mo almacenar los datos.
Guarda la informaciÃ³n junto con metadatos (URL, fecha de captura, etc.).
Gestiona errores en el almacenamiento (por ejemplo, duplicados o inserciones fallidas, o problemas con los indexers).